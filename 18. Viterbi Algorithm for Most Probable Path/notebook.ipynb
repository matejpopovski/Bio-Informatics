{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce355a3cd9847d9f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Day 18 notebook\n",
    "\n",
    "The objectives of this notebook are to practice using a hidden Markov model (HMM) to\n",
    "\n",
    "* simulate sequences\n",
    "* calculate the (log) joint probability of a sequence and path of hidden states\n",
    "* predict a most probable hidden path of states for a sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7c0314d3fbe9a97",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Modules used in this activity\n",
    "import random  # used by sample_categorical\n",
    "import math    # for log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca8cd030ad37a83e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## A `HiddenMarkovModel` class\n",
    "In this activity we will implement a hidden Markov model as a class.  You will be implementing three methods of this class:\n",
    "\n",
    "* one to simulate sequences from the hidden Markov model (`simulate`),\n",
    "* another that computes the (log) joint probability of a hidden state path and observed sequence (`log_joint_probability`), \n",
    "* and one that computes the Viterbi dynamic programming matrix for an observed sequence (`viterbi_matrix`).\n",
    "\n",
    "Like the Markov chain activity, we will be representing each state by a single character and a path of states as a string of state characters.  We will also be representing an observed sequence as a string of characters.  Again, like the Markov chain activity, the transition probability matrix, initial probabilities, and emission probabilty matrix will be indexed by integers corresponding to the indices of the characters within the state string of the model and the characters within the observed character string.  Methods are provided that convert from a string of state or observed characters to a list of indices and vice versa, for your convenience.\n",
    "\n",
    "The hidden Markov Model class that we implement will not explicitly represent a begin state and will instead represent the probability of starting in any given state with an `initial_probs` list of probabilities, as in the `MarkovModel` class.  In addition, we will not represent an end state in our class.\n",
    "\n",
    "Provided are parameters for the occasionally dishonest casino described in the lecture/textbook, which are then used to construct instances of the HiddenMarkovModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5e62beb80d4f0db",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class HiddenMarkovModel:\n",
    "    def __init__(self, states, chars, \n",
    "                 transition_prob_matrix, initial_probs, emission_prob_matrix):\n",
    "        \"\"\"Initializes a HiddenMarkovModel\n",
    "        \n",
    "        Models represented by this class do not explicitly represent a begin state and do\n",
    "        not allow for an end state.\n",
    "        \n",
    "        Args:\n",
    "            states: a string giving the characters representing the hidden states\n",
    "                of the model (1 character per state)\n",
    "            chars: a string giving the set of characters possibly emitted by the\n",
    "                states of the model\n",
    "            transition_prob_matrix: a list of lists of probabilities representing a\n",
    "                transition probability matrix. transition_prob_matrix[s][t] should equal \n",
    "                P(pi_i = t | pi_{i-1} = s). Row s is thus the conditional probability \n",
    "                distribution P(pi_i | pi_{i-1} = s). The indices in this matrix correspond \n",
    "                to the indices of the states in the states argument\n",
    "            initial_probs: a list of probabilities representing the initial state \n",
    "                probabilities. Entry s of this list is P(pi_1 = s), i.e., the probability that\n",
    "                the first hidden state in the chain is s.  The indices of this list correspond to the\n",
    "                indices of the states in the states argument.\n",
    "            emission_prob_matrix: a list of lists of probabilities representing an emission\n",
    "                probability matrix.  emission_prob_matrix[s][c] should equal \n",
    "                P(X_i = c | pi_i = s), i.e., the probability of state s emitting character c. \n",
    "                Row s is thus the conditional probability distribution P(X_i | pi_i = s).\n",
    "                The row indices of this matrix correspond to the indices of the states in\n",
    "                the states argument.  The column indices of the matrix correspond to the \n",
    "                indices of the characters in the chars argument.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.chars = chars\n",
    "        self.transition_prob_matrix = transition_prob_matrix\n",
    "        self.initial_probs = initial_probs\n",
    "        self.emission_prob_matrix = emission_prob_matrix\n",
    "        \n",
    "        # Precompute log-transformations of the model parameters\n",
    "        # to avoid computing these many times\n",
    "        self.log_transition_prob_matrix = log_transform_matrix(transition_prob_matrix)\n",
    "        self.log_initial_probs = log_transform_vector(initial_probs)\n",
    "        self.log_emission_prob_matrix = log_transform_matrix(emission_prob_matrix)\n",
    "    \n",
    "    def encode_states(self, state_sequence):\n",
    "        \"\"\"Encodes a string of state characters as a list of indices of the states.\"\"\"\n",
    "        return [self.states.index(char) for char in state_sequence]\n",
    "\n",
    "    def decode_states(self, indices):\n",
    "        \"\"\"Decodes a sequence of state indices into a string of the state characters.\"\"\"\n",
    "        return \"\".join(self.states[index] for index in indices)\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        \"\"\"Encodes a string of observed characters as a list of indices of the characters.\"\"\"\n",
    "        return [self.chars.index(char) for char in sequence]\n",
    "\n",
    "    def decode_sequence(self, indices):\n",
    "        \"\"\"Decodes a sequence of observed character indices into a string of characters.\"\"\"\n",
    "        return \"\".join(self.chars[index] for index in indices)\n",
    "    \n",
    "    def simulate(self, length):\n",
    "        \"\"\"Simulates a sequence of hidden states and emitted characters of\n",
    "        the given length from this HMM.\n",
    "        \n",
    "        Args:\n",
    "            length: the length of the sequence to simulate\n",
    "        Returns:\n",
    "            A tuple of the form (hidden_state_string, char_string) where hidden_state_string is a\n",
    "            string of state characters and char_string is a string of observed characters.\n",
    "        \"\"\"\n",
    "        ###\n",
    "        ### YOUR CODE HERE\n",
    "        state_indices = [None] * length\n",
    "        char_indices = [None] * length\n",
    "        for i in range(length):\n",
    "            state_probs = self.transition_prob_matrix[state_indices[i - 1]] if i > 0 else self.initial_probs\n",
    "            state_indices[i] = sample_categorical(state_probs)\n",
    "            char_indices[i] = sample_categorical(self.emission_prob_matrix[state_indices[i]])\n",
    "            \n",
    "        return (self.decode_states(state_indices), self.decode_sequence(char_indices))\n",
    "        ###\n",
    "        \n",
    "    def log_joint_probability(self, hidden_state_string, char_string):\n",
    "        \"\"\"Calculates the (natural) log joint probability of a path of hidden states\n",
    "        and an observed sequence given this HMM.\n",
    "        \n",
    "        Args:\n",
    "            hidden_state_string: a string representing the sequence of hidden states (pi)\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            log(P(hidden_states, observed_chars))\n",
    "        \"\"\"\n",
    "        state_indices = self.encode_states(hidden_state_string)\n",
    "        char_indices = self.encode_sequence(char_string)\n",
    "\n",
    "        ###\n",
    "        ### YOUR CODE HERE\n",
    "        log_p = 0.0\n",
    "        last_state_index = None\n",
    "        for state_index, char_index in zip(state_indices, char_indices):\n",
    "            if last_state_index is None:\n",
    "                log_p += self.log_initial_probs[state_index]\n",
    "            else:\n",
    "                log_p += self.log_transition_prob_matrix[last_state_index][state_index]\n",
    "            log_p += self.log_emission_prob_matrix[state_index][char_index]\n",
    "            last_state_index = state_index\n",
    "        return log_p\n",
    "        ###\n",
    "        \n",
    "    def most_probable_path(self, char_string):\n",
    "        \"\"\"Computes a most probable path of hidden states for the observed sequence.\"\"\"\n",
    "        V = self.viterbi_matrix(char_string)\n",
    "        return self.viterbi_traceback(V)\n",
    "        \n",
    "    def viterbi_matrix(self, char_string):\n",
    "        \"\"\"Computes the (log-transformed) Viterbi dynamic programming matrix V for\n",
    "        the given observed sequence.\n",
    "\n",
    "        Args:\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            A matrix (list of lists) representing the Viterbi dynamic programming matrix,\n",
    "            with rows corresponding to states and columns corresponding to positions in the\n",
    "            sequence.\n",
    "        \"\"\"\n",
    "        char_indices = self.encode_sequence(char_string)\n",
    "        \n",
    "        # Initialize the viterbi dynamic programming matrix\n",
    "        # the entry V[k][i] corresponds to the subproblem V_k(i+1)\n",
    "        # where i is a 0-based index (e.g., V[k][0] corresponds to the subproblem\n",
    "        # of the most probable path of the prefix of length = 1). We will not explicitly\n",
    "        # represent the begin or end states.  As a result, we will not explicitly store the\n",
    "        # initialization values described in the textbook and lecture.\n",
    "        V = matrix(len(self.states), len(char_string))\n",
    "        if not char_string: return V\n",
    "        \n",
    "        # initialization (first position in sequence)\n",
    "        for ell in range(len(self.states)):    # loop over hidden state indices\n",
    "            V[ell][0] = (self.log_initial_probs[ell] + \n",
    "                         self.log_emission_prob_matrix[ell][char_indices[0]])\n",
    "\n",
    "        # main fill stage\n",
    "        for i in range(1, len(char_string)):    # loop over positions\n",
    "            for ell in range(len(self.states)): # loop over hidden state indices\n",
    "                ###\n",
    "                ### YOUR CODE HERE\n",
    "                V[ell][i] = (self.log_emission_prob_matrix[ell][char_indices[i]] + \n",
    "                             max(V[k][i - 1] + self.log_transition_prob_matrix[k][ell]\n",
    "                                 for k in range(len(self.states))))\n",
    "                ###\n",
    "\n",
    "        return V\n",
    "    \n",
    "    def viterbi_traceback(self, V):\n",
    "        \"\"\"Computes a most probable path given a (log) Viterbi dynamic programming matrix.\n",
    "        \n",
    "        Uses a traceback procedure that does not require traceback pointers.  In the case of\n",
    "        ties, this traceback prefers the state with the largest index.\n",
    "        \n",
    "        Args:\n",
    "            V: A matrix (list of lists) representing the Viterbi dynamic programming matrix\n",
    "               containing log-transformed values.\n",
    "        Returns:\n",
    "            A string representing a most probable sequence of hidden states\n",
    "        \"\"\"\n",
    "        L = len(V[0])               # deduce the length of the sequence from # columns in V\n",
    "        if L == 0: return \"\"        # empty string base case\n",
    "        state_indices = [None] * L  # initialize hidden state path\n",
    "        \n",
    "        # determine the state at the last position in a most probable path\n",
    "        max_prob, max_state = max((V[k][L - 1], k) for k in range(len(self.states)))\n",
    "        state_indices[L - 1] = max_state\n",
    "        \n",
    "        # traceback from this last state by redoing the recurrence calculation at each step.\n",
    "        # the emission probabilities are not included in the calculations because they are\n",
    "        # irrelevant for determining the maximizing state\n",
    "        for i in range(L - 1, 0, -1):\n",
    "            max_prob, max_state = max((V[k][i - 1] + self.log_transition_prob_matrix[k][max_state], k)\n",
    "                                      for k in range(len(self.states)))\n",
    "            state_indices[i - 1] = max_state\n",
    "            \n",
    "        # return string representation of hidden state path\n",
    "        return self.decode_states(state_indices)\n",
    "\n",
    "def log_transform_vector(v):\n",
    "    \"\"\"Returns a new vector (a list) with log-transformed values\"\"\"\n",
    "    return list(map(math.log, v))\n",
    "\n",
    "def log_transform_matrix(m):\n",
    "    \"\"\"Returns a new matrix (a list of lists) with log-transformed values\"\"\"\n",
    "    return list(map(log_transform_vector, m))\n",
    "\n",
    "def round_matrix(m, digits=2):\n",
    "    \"\"\"Returns a new matrix (a list of lists) with rounded values\"\"\"\n",
    "    return [round_vector(v, digits) for v in m]\n",
    "    \n",
    "def round_vector(v, digits=2):\n",
    "    \"\"\"Returns a new vector (a list) with rounded values\"\"\"\n",
    "    return [round(x, digits) for x in v]\n",
    "\n",
    "def matrix(num_rows, num_cols, initial_value=None):\n",
    "    \"\"\"Constructs a matrix (a list of lists)\"\"\"\n",
    "    return [[initial_value] * num_cols for i in range(num_rows)]\n",
    "\n",
    "# Using the class above, we construct an HMM for the occasionally dishonest casino example\n",
    "# described in the lecture and textbook\n",
    "casino_states = \"FL\"     # F = fair die, L = loaded die\n",
    "casino_chars = \"123456\"  # the six sides of the die\n",
    "casino_initial_probs = [0.5, 0.5]\n",
    "casino_transition_prob_matrix = [\n",
    "    [0.95, 0.05],\n",
    "    [0.10, 0.90]\n",
    "]\n",
    "\n",
    "casino_emission_prob_matrix = [\n",
    "    [ 1/6,  1/6,  1/6,  1/6,  1/6, 1/6],\n",
    "    [1/10, 1/10, 1/10, 1/10, 1/10, 1/2]\n",
    "]\n",
    "casino_hmm = HiddenMarkovModel(casino_states, \n",
    "                               casino_chars, \n",
    "                               casino_transition_prob_matrix, \n",
    "                               casino_initial_probs,\n",
    "                               casino_emission_prob_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-797a2dd5846166b2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Below is a function that you will need to use in implementing the `simulate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f97667e23591dc3e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_categorical(distribution):\n",
    "    \"\"\"Randomly sample from a categorical distribution (a discrete distribution over K categories).\n",
    "    \n",
    "    Args:\n",
    "        distribution: a list of probabilities representing a discrete distribution over K categories.\n",
    "    Returns:\n",
    "        The index of the category sampled.\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    for i, prob in enumerate(distribution):\n",
    "        if r < prob:\n",
    "            return i\n",
    "        else:\n",
    "            r -= prob\n",
    "    # in case we encounter floating point issues return the last index\n",
    "    return len(distribution) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67cb56d50a836a1f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## PROBLEM 1: Simulate a hidden state path and sequence from an HMM (1 POINT)\n",
    "\n",
    "Implement the `simulate` method of the `HiddenMarkovModel` class.  You should call the `sample_categorical` function provided to you above to sample each state and emission.  *IMPORTANT IMPLEMENTATION NOTE:* you should simulate the random variables in the following order so that you may pass the tests: $\\pi_1, x_1, \\pi_2, x_2, \\ldots, \\pi_L, x_L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "simulate",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: simulate passed all tests!\n"
     ]
    }
   ],
   "source": [
    "# tests for simulate\n",
    "random.seed(8)\n",
    "assert casino_hmm.simulate(1) == ('F', '6')\n",
    "random.seed(8)\n",
    "assert casino_hmm.simulate(2) == ('FF', '65')\n",
    "random.seed(8)\n",
    "assert casino_hmm.simulate(4) == ('FFFL', '6523')\n",
    "random.seed(8)\n",
    "assert casino_hmm.simulate(10) == ('FFFLLLLFFF', '6523556226')\n",
    "random.seed(17)\n",
    "assert casino_hmm.simulate(10) == ('LLLLFFFFFF', '6362322665')\n",
    "print(\"SUCCESS: simulate passed all tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-748476a4252cfef0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## PROBLEM 2: Calculate the (log)  joint probability of a hidden path of states and an observed sequence given a hidden Markov model (1 POINT)\n",
    "\n",
    "Implement the `log_joint_probability` method of the `HiddenMarkovModel` class.  To avoid numerical issues, be sure to implement this as a sum of log-transformed probability parameters from the model. If you implement this by taking the logarithm of the product of the probabilities, you will run into numerical problems for long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "log_joint_probability",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: log_joint_probability passed all tests!\n"
     ]
    }
   ],
   "source": [
    "# tests for log_joint_probability\n",
    "assert round(casino_hmm.log_joint_probability('F', '6'), 2) == -2.48\n",
    "assert round(casino_hmm.log_joint_probability('L', '1'), 2) == -3.0\n",
    "assert round(casino_hmm.log_joint_probability('FL', '35'), 2) == -7.78\n",
    "assert round(casino_hmm.log_joint_probability('LF', '24'), 2) == -7.09\n",
    "assert round(casino_hmm.log_joint_probability('LL', '24'), 2) == -5.4\n",
    "assert round(casino_hmm.log_joint_probability('LFL', '246'), 2) == -10.78\n",
    "assert round(casino_hmm.log_joint_probability('FFFLLLLFFF', '6523556226'), 2) == -24.86\n",
    "assert round(casino_hmm.log_joint_probability('FL' * 100, '16' * 100), 2) == -776.71\n",
    "print(\"SUCCESS: log_joint_probability passed all tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8b57076554948bd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## PROBLEM 3: Computing the Viterbi dynamic programming matrix (1 POINT)\n",
    "\n",
    "Implement the `viterbi_matrix` method of the `HiddenMarkovModel` class, which computes the (log) Viterbi dynamic programming matrix given an observed sequence.  You do not need to keep track of traceback pointers.  Provided is a traceback method that does not require traceback pointers.\n",
    "\n",
    "Your Viterbi matrix should use log-transformed values to avoid numerical issues.  Note that the `HiddenMarkovModel` class precomputes log-transformed parameters, which you should use for convenience and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "viterbi_matrix",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: viterbi_matrix passed all tests!\n"
     ]
    }
   ],
   "source": [
    "# tests for viterbi_matrix\n",
    "assert round_matrix(casino_hmm.viterbi_matrix('6')) == [[-2.48], \n",
    "                                                        [-1.39]]\n",
    "assert round_matrix(casino_hmm.viterbi_matrix('1')) == [[-2.48], \n",
    "                                                        [-3.0]]\n",
    "assert round_matrix(casino_hmm.viterbi_matrix('16')) == [[-2.48, -4.33], \n",
    "                                                         [-3.0, -3.79]]\n",
    "assert round_matrix(casino_hmm.viterbi_matrix('165')) == [[-2.48, -4.33, -6.17], \n",
    "                                                          [-3.0, -3.79, -6.2]]\n",
    "assert round_matrix(casino_hmm.viterbi_matrix('666661111')) == [\n",
    "    [-2.48, -4.33, -6.17, -7.08, -7.88, -8.67, -10.52, -12.36, -14.2],\n",
    "    [-1.39, -2.18, -2.98, -3.78, -4.58, -6.99,   -9.4,  -11.8, -14.21]]\n",
    "assert round_matrix(casino_hmm.viterbi_matrix('4631262516')) == [\n",
    "    [-2.48, -4.33, -6.17, -8.01, -9.86, -11.7, -13.54, -15.39, -17.23, -19.07],\n",
    "    [-3.0, -3.79, -6.2, -8.61, -11.02, -11.82, -14.22, -16.63, -19.04, -19.84]]\n",
    "print(\"SUCCESS: viterbi_matrix passed all tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b223f23bf0a458c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exploration activity: how well does the most probable path predict the true path of hidden states?\n",
    "\n",
    "Now that you have successfully implemented the Viterbi algorithm, try simulating a large number of sequences from the casino HMM and see how well the most probable path (obtained by calling the `most_probable_path` method) matches the true path of hidden states.  How accurate is the most probable path in predicting the truth when the model we are using for prediction is the same as the model used for simulation?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b248d1ed2be88cb6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### Your thoughts here\n",
    "###\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
